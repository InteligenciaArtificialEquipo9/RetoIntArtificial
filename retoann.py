# -*- coding: utf-8 -*-
"""retoANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CXCIfvoOj6GSr4Df_ySzTnVkQIx3G2cT
"""

import pandas as pd 
import numpy as np 
import seaborn as sns
import sklearn
import csv
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn import metrics

import matplotlib.pyplot as plt

# Línea a cambiar para correr
ruta_al_archivo = "/train.csv"

df = pd.read_csv(ruta_al_archivo)
df

ruta_al_archivo = "/test.csv"

test = pd.read_csv(ruta_al_archivo)
test



"""Limpieza de datos """

missing_values_count2 = df.isnull().sum()
print(missing_values_count2)
missing_values_count3 = test.isnull().sum()
print(missing_values_count3)
#Vemos que en ambos dataset hay valores faltantes: age y cabin sobresalen más

#Rellenamos la edad

df_sin_nulos= df.copy()
df_sin_nulos.dropna(subset=['Age'], inplace=True)
test_sin_nulos= test.copy()
test_sin_nulos.dropna(subset=['Age'], inplace=True)
test_sin_nulos["Fare"].fillna(round(test_sin_nulos["Fare"].mean(),3), inplace = True)

df_sin_nulos2=df_sin_nulos.drop(["Survived"], axis=1)

completo_sin_nulos = np.concatenate((df_sin_nulos2, test_sin_nulos))

completo_sin_nulos2 = pd.DataFrame(completo_sin_nulos)
completo_sin_nulos2.columns= ["PassengerId", "Pclass", "Name", "Sex", "Age", "SibSp", "Parch", "Ticket", "Fare", "Cabin", "Embarked"]

X_train1= df_sin_nulos[[ "SibSp", "Parch", "Fare"]]
y_train1 = np.ravel(df_sin_nulos[["Age"]].astype(int))

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression


X_train, X_test, y_train, y_test = train_test_split(X_train1, y_train1,
                                  train_size=0.8)

#juntar los dos dataset para sacar el modelo entrenado
model1 = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(50, ), max_iter=1000, learning_rate_init=0.001, tol=1e-4, activation='relu', verbose=10)
model1.fit(X_train, y_train)
#model1 = DecisionTreeClassifier().fit(X_train, y_train)
#model1 = LogisticRegression(solver='liblinear', random_state=0)
#model1.fit(X_train, y_train)
newdf= df.copy()
newdf_test = test.copy()
newdf = newdf.fillna(-1)
newdf_test = newdf_test.fillna(-1)

newdf2 = newdf.loc[newdf['Age'] == -1.0]
newdf2_test = newdf_test.loc[newdf['Age'] == -1.0]

X_test1 = newdf2[["SibSp", "Parch", "Fare"]]
X_test_test = newdf2_test[["SibSp", "Parch", "Fare"]]
#hacer los predcit por aparte 
y_pred = model1.predict(X_test1)
y_pred_test = model1.predict(X_test_test)
y_pred_b = model1.predict(X_test)
print(accuracy_score(y_test, y_pred_b))

newdf2["Age"] = y_pred
newdf2_test["Age"] = y_pred_test

completo = np.concatenate((newdf2, df_sin_nulos))
completo_test = np.concatenate((newdf2_test, test_sin_nulos))
completo2_test = pd.DataFrame(completo_test)
#unirlos los dos diferentes 
completo2 = pd.DataFrame(completo)
completo2.columns= ["PassengerId", "Survived", "Pclass", "Name", "Sex", "Age", "SibSp", "Parch", "Ticket", "Fare", "Cabin", "Embarked"]
completo2_test.columns = ["PassengerId", "Pclass", "Name", "Sex", "Age", "SibSp", "Parch", "Ticket", "Fare", "Cabin", "Embarked"]
test = completo2_test.copy()

#Hacemos una nueva columna del tamaño de las familias

completo2['Family'] = completo2['SibSp'] + completo2['Parch']

sns.barplot(completo2['Family'], completo2["Survived"])
#Es más probable que sobrevivas con familias de 3 personas

missing_values_count2 = completo2.isnull().sum()
print(missing_values_count2)
#Solo nos faltan los datos de las Cabinas y de que puerto embarcaron

"""Gráficas"""

sns.barplot(df["Sex"], df["Survived"])
#Las mujeres tienen más probabilidad de sobrevivir

sns.barplot(df["Pclass"], df["Survived"])
# Por el plan de evacuación que se llevó a cabo, hubo más sobrevivientes de primera clase

#Distribución de la edad
plt.hist(df["Age"])

"""Valores atípicos """

Q_1 = dict(df.describe()["Age"])['25%']
Q_3 = dict(df.describe()["Age"])['75%']
IQR = Q_3 - Q_1
lim_sup = Q_3+1.5*IQR
lim_inf = Q_1-1.5*IQR

plt.boxplot(df["Age"].values[~np.isnan(df["Age"].values)], vert = False)
plt.axvline(lim_sup, color = 'green')
plt.axvline(lim_inf, color = 'blue')
plt.title( "Age")
  
plt.show()
# En este caso no imputaremos los datos atípicos

"""modelo:"""

completo2['Sex'].replace("male", 0,inplace=True) 
completo2['Sex'].replace("female", 1,inplace=True) 


completo2['Age'] = completo2['Age'].astype(int)
completo2.loc[ completo2['Age'] <= 11, 'Age'] = 0
completo2.loc[(completo2['Age'] > 11) & (completo2['Age'] <= 18), 'Age'] = 1
completo2.loc[(completo2['Age'] > 18) & (completo2['Age'] <= 22), 'Age'] = 2
completo2.loc[(completo2['Age'] > 22) & (completo2['Age'] <= 27), 'Age'] = 3
completo2.loc[(completo2['Age'] > 27) & (completo2['Age'] <= 33), 'Age'] = 4
completo2.loc[(completo2['Age'] > 33) & (completo2['Age'] <= 40), 'Age'] = 5
completo2.loc[(completo2['Age'] > 40) & (completo2['Age'] <= 66), 'Age'] = 6
completo2.loc[ completo2['Age'] > 66, 'Age'] = 6


X = completo2[["Pclass","Sex", "Age","Family"]]
y = completo2[["Survived"]].astype('int')

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                  train_size=0.8)

# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
model = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(300, ), max_iter=1000, learning_rate_init=0.001, tol=1e-4, activation='relu', verbose=10)
model.fit(X_train, y_train)


"""
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

tree = DecisionTreeClassifier()
model = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,
                        random_state=1)

model.fit(X_train, y_train)
"""
"""
N_TRAIN_SAMPLES = X_train.shape[0]
N_EPOCHS = 45
N_BATCH = 8
N_CLASSES = np.unique(y_train)

scores_train = []
scores_test = []

### Train epoch by epoch

# EPOCH
epoch = 0
while epoch < N_EPOCHS:
    print('epoch: ', epoch)
    # SHUFFLING
    random_perm = np.random.permutation(X_train.shape[0])
    mini_batch_index = 0
    while True:
        # MINI-BATCH
        indices = random_perm[mini_batch_index:mini_batch_index + N_BATCH]
        model.partial_fit(X_train[indices, :], y_train[indices], classes=N_CLASSES)
        mini_batch_index += N_BATCH

        if mini_batch_index >= N_TRAIN_SAMPLES:
            break

    # SCORE TRAIN
    scores_train.append(model.score(X_train, y_train))

    # SCORE TEST
    scores_test.append(model.score(X_test, y_test))

    epoch += 1

### Plot Accuracy results

# Plot 
fig, ax = plt.subplots(2, sharex=True, sharey=True)
ax[0].plot(scores_train)
ax[0].set_title('Train')
ax[1].plot(scores_test)
ax[1].set_title('Test')
fig.suptitle("Accuracy over epochs", fontsize=14)
plt.show()

"""

y_pred = model.predict(X_test)
y_scores = model.predict_proba(X_test)

accuracy_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred )

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')
plt.show()

# compute true positive rate and false positive rate

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
# plotting them against each other
def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):
    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'r', linewidth=4)
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate (FPR)', fontsize=16)
    plt.ylabel('True Positive Rate (TPR)', fontsize=16)

plt.figure(figsize=(12, 6))
plot_roc_curve(false_positive_rate, true_positive_rate)
plt.show()

#ROC
#https://www.datatechnotes.com/2019/11/how-to-create-roc-curve-in-python.html
predY = model.predict_proba(X_test)


fpr,tpr, thresh = metrics.roc_curve(y_test, predY[:,1])

auc = metrics.auc(fpr, tpr)
print("AUC:", auc)

plt.plot(fpr, tpr, label='ROC curve (area = %.2f)' %auc)
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.grid()
plt.legend()
plt.show()

test['Sex'].replace("male", 0,inplace=True) 
test['Sex'].replace("female", 1,inplace=True) 

test["Age"].fillna(round(test["Age"].mean(),3), inplace = True)


test['Age'] = test['Age'].astype(int)
test.loc[ test['Age'] <= 11, 'Age'] = 0
test.loc[(test['Age'] > 11) & (test['Age'] <= 18), 'Age'] = 1
test.loc[(test['Age'] > 18) & (test['Age'] <= 22), 'Age'] = 2
test.loc[(test['Age'] > 22) & (test['Age'] <= 27), 'Age'] = 3
test.loc[(test['Age'] > 27) & (test['Age'] <= 33), 'Age'] = 4
test.loc[(test['Age'] > 33) & (test['Age'] <= 40), 'Age'] = 5
test.loc[(test['Age'] > 40) & (test['Age'] <= 66), 'Age'] = 6
test.loc[ test['Age'] > 66, 'Age'] = 6


test['Family'] = test['SibSp'] + test['Parch']


test["Fare"].fillna(round(test["Fare"].mean(),3), inplace = True)


X_test_Kaggle = test[["Pclass","Sex", "Age","Family"]]

y_pred_kaggle = model.predict(X_test_Kaggle)
print(y_pred)

ids = np.array(test[["PassengerId"]].to_numpy())
y_pred_kaggle = np.array(pd.DataFrame(y_pred_kaggle).to_numpy())

aa = ids+ y_pred_kaggle


df = pd.DataFrame({
    'PassengerId':ids.flatten(),
    'Survived':y_pred_kaggle.flatten()
})
df

df.to_csv('example.csv')

#files.download('example.csv')